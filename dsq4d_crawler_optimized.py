#!/usr/bin/env python
# -*- coding: utf-8 -*-

import requests
import sqlite3
import re
import time
import random
import argparse
import os
import sys
import base64
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
from bs4 import BeautifulSoup
from tqdm import tqdm
from datetime import datetime
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# å…¨å±€å˜é‡
BASE_URL = "https://m.dsq4d.com"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
    "Referer": "https://m.dsq4d.com/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}

# åˆ†ç±»IDå’Œåç§°æ˜ å°„
CATEGORIES = {
    1: "ç”µå½±",
    2: "ç”µè§†å‰§", 
    3: "åŠ¨æ¼«",
    4: "ç»¼è‰º",
    19: "å¤§é™†å‰§",
    20: "æ¬§ç¾å‰§", 
    21: "é¦™æ¸¯å‰§",
    22: "éŸ©å›½å‰§",    
    23: "å°æ¹¾å‰§",
    24: "æ—¥æœ¬å‰§", 
    25: "æµ·å¤–å‰§",
    26: "æ³°å›½å‰§",  
    27: "çŸ­å‰§"
}

# æ•°æ®åº“æ–‡ä»¶
DB_FILE = "dy.db"

class OptimizedDSQ4DCrawler:
    def __init__(self, test_mode=False, delay=0.1, max_workers=8, batch_size=50):
        """åˆå§‹åŒ–ä¼˜åŒ–çˆ¬è™«"""
        self.test_mode = test_mode
        self.delay = delay
        self.max_workers = max_workers
        self.batch_size = batch_size
        
        # åˆ›å»ºä¼˜åŒ–çš„session
        self.session = self._create_optimized_session()
        
        # æ•°æ®åº“è¿æ¥æ± 
        self.db_lock = threading.Lock()
        self.conn = sqlite3.connect(DB_FILE, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        
        # æ‰¹é‡æ“ä½œç¼“å­˜
        self.movie_batch = []
        self.m3u8_batch = []
        self.batch_lock = threading.Lock()
        
        self._ensure_tables()
        
    def _create_optimized_session(self):
        """åˆ›å»ºä¼˜åŒ–çš„HTTPä¼šè¯"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.3,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        # é…ç½®é€‚é…å™¨
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=20,
            pool_maxsize=20,
            pool_block=False
        )
        
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update(HEADERS)
        
        return session
    
    def _ensure_tables(self):
        """ç¡®ä¿æ‰€éœ€çš„æ•°æ®åº“è¡¨å·²åˆ›å»º"""
        tables = ["dy", "m3u8", "crawl_progress"]
        for table in tables:
            cursor = self.conn.cursor()
            cursor.execute(f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table}'")
            if not cursor.fetchone():
                print(f"è¡¨ {table} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ init_db.py åˆå§‹åŒ–æ•°æ®åº“")
                sys.exit(1)
            cursor.close()
    
    def _get_with_retry(self, url, timeout=5):
        """å‘é€GETè¯·æ±‚ï¼Œå¸¦ä¼˜åŒ–çš„é‡è¯•æœºåˆ¶"""
        try:
            response = self.session.get(url, timeout=timeout)
            if response.status_code == 200:
                return response
            else:
                print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼ŒURL: {url}")
        except Exception as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}, URL: {url}")
        return None
    
    def _smart_delay(self):
        """æ™ºèƒ½å»¶è¿Ÿï¼Œæ ¹æ®å¹¶å‘æ•°åŠ¨æ€è°ƒæ•´"""
        if self.delay > 0:
            delay = self.delay * (0.8 + random.random() * 0.4)
            time.sleep(delay)
    
    def check_movie_exists(self, dyid):
        """æ£€æŸ¥å½±ç‰‡æ˜¯å¦å·²å­˜åœ¨äºdyè¡¨ä¸­"""
        with self.db_lock:
            cursor = self.conn.cursor()
            try:
                cursor.execute("SELECT dyid FROM dy WHERE dyid = ?", (dyid,))
                result = cursor.fetchone()
                return result is not None
            finally:
                cursor.close()
    
    def get_existing_m3u8_play_urls(self, dyid):
        """è·å–æŒ‡å®šdyidå·²å­˜åœ¨çš„play_urlåˆ—è¡¨"""
        with self.db_lock:
            cursor = self.conn.cursor()
            try:
                cursor.execute("SELECT play_url FROM m3u8 WHERE dyid = ? AND m3u8_url IS NOT NULL", (dyid,))
                results = cursor.fetchall()
                return [row[0] for row in results] if results else []
            finally:
                cursor.close()
    
    def get_missing_episodes(self, dyid, total_episodes):
        """è·å–ç¼ºå¤±çš„é›†æ•°ä¿¡æ¯"""
        with self.db_lock:
            cursor = self.conn.cursor()
            try:
                cursor.execute("""
                SELECT episode FROM m3u8 
                WHERE dyid = ? AND m3u8_url IS NOT NULL 
                ORDER BY episode
                """, (dyid,))
                existing_episodes = [row[0] for row in cursor.fetchall()]
                all_episodes = set(range(1, total_episodes + 1))
                missing_episodes = list(all_episodes - set(existing_episodes))
                return sorted(missing_episodes)
            finally:
                cursor.close()
    
    def get_total_pages(self, category_id):
        """è·å–åˆ†ç±»çš„æ€»é¡µæ•°"""
        url = f"{BASE_URL}/list/{category_id}-1.html"
        response = self._get_with_retry(url)
        if not response:
            print(f"è·å–åˆ†ç±» {category_id} çš„æ€»é¡µæ•°å¤±è´¥")
            return 0
            
        soup = BeautifulSoup(response.text, 'lxml')
        
        # ä¼˜å…ˆæŸ¥æ‰¾å°¾é¡µé“¾æ¥
        last_page_link = soup.select_one('a:-soup-contains("å°¾é¡µ")')
        if last_page_link and 'href' in last_page_link.attrs:
            href = str(last_page_link['href'])
            match = re.search(r'/list/\d+-(\d+)\.html', href)
            if match:
                return int(match.group(1))
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾æ‰€æœ‰é¡µç é“¾æ¥
        page_links = soup.select('ul.page a')
        max_page = 0
        for link in page_links:
            text = link.get_text(strip=True)
            if text.isdigit():
                page_num = int(text)
                if page_num > max_page:
                    max_page = page_num
        
        return max_page if max_page > 0 else 1
    
    def get_movie_links_batch(self, category_id, pages):
        """æ‰¹é‡è·å–å¤šé¡µçš„å½±ç‰‡é“¾æ¥"""
        all_links = []
        
        def fetch_page_links(page):
            url = f"{BASE_URL}/list/{category_id}-{page}.html"
            response = self._get_with_retry(url)
            if not response:
                return []
            
            soup = BeautifulSoup(response.text, 'lxml')
            movie_links = []
            
            # æŸ¥æ‰¾å½±ç‰‡åˆ—è¡¨
            vodlist_ul = soup.select_one('ul.list_mov') or soup.select_one('ul[class*="-vodlist"]')
            if vodlist_ul:
                vodlist = vodlist_ul.select('li a')
                for link in vodlist:
                    if 'href' in link.attrs:
                        href = str(link['href'])
                        if href.startswith('/mp4/'):
                            full_url = f"{BASE_URL}{href}"
                            movie_links.append(full_url)
            
            return list(dict.fromkeys(movie_links))
        
        # å¹¶å‘è·å–å¤šé¡µé“¾æ¥
        with ThreadPoolExecutor(max_workers=min(len(pages), 5)) as executor:
            future_to_page = {executor.submit(fetch_page_links, page): page for page in pages}
            
            for future in as_completed(future_to_page):
                page = future_to_page[future]
                try:
                    links = future.result()
                    all_links.extend(links)
                    print(f"é¡µé¢ {page}: è·å–åˆ° {len(links)} ä¸ªé“¾æ¥")
                except Exception as e:
                    print(f"è·å–é¡µé¢ {page} é“¾æ¥å¤±è´¥: {e}")
        
        return list(dict.fromkeys(all_links))
    
    def parse_movie_detail_fast(self, url):
        """å¿«é€Ÿè§£æå½±ç‰‡è¯¦æƒ…"""
        response = self._get_with_retry(url, timeout=3)
        if not response:
            return None
        
        # æå–dyid
        match = re.search(r'/mp4/(\d+)\.html', url)
        if not match:
            return None
        dyid = int(match.group(1))
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # å¿«é€Ÿæå–åŸºæœ¬ä¿¡æ¯
        title_elem = soup.select_one('h1.title')
        name = title_elem.get_text(strip=True).split('(')[0].strip() if title_elem else "æœªçŸ¥"
        
        # æ‰¹é‡æå–æ•°æ®å…ƒç´ 
        data_elems = soup.select('p.data')
        type_text = region_text = year_text = actors_text = directors_text = "æœªçŸ¥"
        
        if data_elems:
            # ç±»å‹ã€åœ°åŒºã€å¹´ä»½
            if len(data_elems) > 0:
                type_links = data_elems[0].select('a')
                if type_links:
                    type_text = type_links[0].get_text(strip=True)
                    if len(type_links) > 1:
                        region_text = type_links[1].get_text(strip=True)
                    if len(type_links) > 2:
                        year_text = type_links[2].get_text(strip=True)
            
            # æ¼”å‘˜
            if len(data_elems) > 1:
                actor_links = data_elems[1].select('a')
                if actor_links:
                    actors_text = ", ".join([a.get_text(strip=True) for a in actor_links])
            
            # å¯¼æ¼”
            if len(data_elems) > 2:
                director_links = data_elems[2].select('a')
                if director_links:
                    directors_text = ", ".join([d.get_text(strip=True) for d in director_links])
        
        # å¿«é€Ÿæå–ç®€ä»‹
        description = "æš‚æ— ç®€ä»‹"
        desc_elems = soup.select('div[class*="-content__desc"]')
        if len(desc_elems) > 1:
            description = desc_elems[1].get_text(strip=True)
        
        if description == "æš‚æ— ç®€ä»‹":
            meta_desc = soup.select_one('meta[name="description"]')
            if meta_desc and 'content' in meta_desc.attrs:
                content = str(meta_desc['content'])
                match = re.search(r'å‰§æƒ…[:ï¼š](.+)', content)
                if match:
                    description = match.group(1).strip()
                else:
                    description = content.strip()
        
        return {
            'dyid': dyid,
            'name': name,
            'type': type_text,
            'region': region_text,
            'year': year_text,
            'actors': actors_text,
            'directors': directors_text,
            'description': description,
            'url': url
        }
    
    def get_episode_count_fast(self, soup):
        """ä»å·²è§£æçš„soupä¸­å¿«é€Ÿè·å–é›†æ•°"""
        playlist_ul = soup.select_one('ul[class*="-content__playlist"]')
        if not playlist_ul:
            return 1
        
        playlist_items = playlist_ul.select('li a')
        episode_count = 0
        for item in playlist_items:
            if item.get_text(strip=True) != "APPæ’­æ”¾":
                episode_count += 1
        
        return max(episode_count, 1)
    
    def get_m3u8_urls_batch(self, dyid, episode_count, movie_name):
        """æ‰¹é‡è·å–m3u8é“¾æ¥"""
        m3u8_data = []
        
        def fetch_m3u8(episode_index):
            play_url = f"{BASE_URL}/play/{dyid}-0-{episode_index}.html"
            response = self._get_with_retry(play_url, timeout=3)
            if not response:
                return episode_index, play_url, None
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¿«é€ŸæŸ¥æ‰¾m3u8é“¾æ¥
            content = response.text
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾player_aaaaé…ç½®
            player_match = re.search(r'var player_aaaa\s*=\s*({.*?})', content, re.DOTALL)
            if player_match:
                player_data = player_match.group(1)
                url_match = re.search(r"url\s*:\s*'([^']*)'", player_data)
                if url_match:
                    url = url_match.group(1)
                    
                    if "get_dplayer" in url:
                        decrypt_api_url = f"{BASE_URL}{url}"
                        api_response = self._get_with_retry(decrypt_api_url, timeout=3)
                        if api_response:
                            try:
                                api_data = api_response.json()
                                if api_data.get('code') == 200 and api_data.get('url'):
                                    return episode_index, play_url, api_data['url']
                            except:
                                pass
                    elif re.match(r'^[A-Za-z0-9+/=]+$', url) and len(url) > 20:
                        try:
                            decoded_url = base64.b64decode(url).decode('utf-8')
                            if decoded_url.startswith('http'):
                                return episode_index, play_url, decoded_url
                        except:
                            pass
                    elif url.startswith('http'):
                        return episode_index, play_url, url
            
            # æ–¹æ³•2ï¼šç›´æ¥æŸ¥æ‰¾m3u8é“¾æ¥
            m3u8_match = re.search(r'(https?://[^\s\'"`,]+\.m3u8)', content)
            if m3u8_match:
                return episode_index, play_url, m3u8_match.group(1)
            
            return episode_index, play_url, None
        
        # å¹¶å‘è·å–m3u8é“¾æ¥
        with ThreadPoolExecutor(max_workers=min(episode_count, 5)) as executor:
            future_to_episode = {
                executor.submit(fetch_m3u8, i): i 
                for i in range(episode_count)
            }
            
            for future in as_completed(future_to_episode):
                try:
                    episode_index, play_url, m3u8_url = future.result()
                    m3u8_data.append({
                        'dyid': dyid,
                        'name': movie_name,
                        'episode': episode_index + 1,
                        'play_url': play_url,
                        'm3u8_url': m3u8_url
                    })
                except Exception as e:
                    episode_index = future_to_episode[future]
                    print(f"è·å–ç¬¬{episode_index + 1}é›†m3u8å¤±è´¥: {e}")
        
        return sorted(m3u8_data, key=lambda x: x['episode'])
    
    def get_m3u8_urls_selective(self, dyid, episode_numbers, movie_name):
        """é€‰æ‹©æ€§è·å–æŒ‡å®šé›†æ•°çš„m3u8é“¾æ¥"""
        m3u8_data = []
        
        def fetch_m3u8(episode_number):
            episode_index = episode_number - 1  # è½¬æ¢ä¸º0åŸºç´¢å¼•
            play_url = f"{BASE_URL}/play/{dyid}-0-{episode_index}.html"
            response = self._get_with_retry(play_url, timeout=3)
            if not response:
                return episode_number, play_url, None
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¿«é€ŸæŸ¥æ‰¾m3u8é“¾æ¥
            content = response.text
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾player_aaaaé…ç½®
            player_match = re.search(r'var player_aaaa\s*=\s*({.*?})', content, re.DOTALL)
            if player_match:
                player_data = player_match.group(1)
                url_match = re.search(r"url\s*:\s*'([^']*)'", player_data)
                if url_match:
                    url = url_match.group(1)
                    
                    if "get_dplayer" in url:
                        decrypt_api_url = f"{BASE_URL}{url}"
                        api_response = self._get_with_retry(decrypt_api_url, timeout=3)
                        if api_response:
                            try:
                                api_data = api_response.json()
                                if api_data.get('code') == 200 and api_data.get('url'):
                                    return episode_number, play_url, api_data['url']
                            except:
                                pass
                    elif re.match(r'^[A-Za-z0-9+/=]+$', url) and len(url) > 20:
                        try:
                            decoded_url = base64.b64decode(url).decode('utf-8')
                            if decoded_url.startswith('http'):
                                return episode_number, play_url, decoded_url
                        except:
                            pass
                    elif url.startswith('http'):
                        return episode_number, play_url, url
            
            # æ–¹æ³•2ï¼šç›´æ¥æŸ¥æ‰¾m3u8é“¾æ¥
            m3u8_match = re.search(r'(https?://[^\s\'"`,]+\.m3u8)', content)
            if m3u8_match:
                return episode_number, play_url, m3u8_match.group(1)
            
            return episode_number, play_url, None
        
        # å¹¶å‘è·å–æŒ‡å®šé›†æ•°çš„m3u8é“¾æ¥
        with ThreadPoolExecutor(max_workers=min(len(episode_numbers), 5)) as executor:
            future_to_episode = {
                executor.submit(fetch_m3u8, episode_num): episode_num 
                for episode_num in episode_numbers
            }
            
            for future in as_completed(future_to_episode):
                try:
                    episode_number, play_url, m3u8_url = future.result()
                    m3u8_data.append({
                        'dyid': dyid,
                        'name': movie_name,
                        'episode': episode_number,
                        'play_url': play_url,
                        'm3u8_url': m3u8_url
                    })
                except Exception as e:
                    episode_number = future_to_episode[future]
                    print(f"è·å–ç¬¬{episode_number}é›†m3u8å¤±è´¥: {e}")
        
        return sorted(m3u8_data, key=lambda x: x['episode'])
    
    def batch_save_to_db(self, movies=None, m3u8s=None):
        """æ‰¹é‡ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“ï¼ˆä¼˜åŒ–æŸ¥é‡ï¼‰"""
        with self.db_lock:
            cursor = self.conn.cursor()
            try:
                new_movies = 0
                updated_movies = 0
                new_m3u8s = 0
                updated_m3u8s = 0
                
                if movies:
                    # æ‰¹é‡æ’å…¥æˆ–æ›´æ–°å½±ç‰‡ä¿¡æ¯ï¼ˆåªå¤„ç†æ–°å½±ç‰‡ï¼‰
                    for movie in movies:
                        cursor.execute("SELECT dyid FROM dy WHERE dyid = ?", (movie['dyid'],))
                        if cursor.fetchone():
                            # æ›´æ–°ç°æœ‰å½±ç‰‡ä¿¡æ¯
                            cursor.execute("""
                            UPDATE dy SET 
                                name = ?, type = ?, region = ?, year = ?, 
                                actors = ?, directors = ?, description = ?, url = ?,
                                crawl_time = CURRENT_TIMESTAMP
                            WHERE dyid = ?
                            """, (
                                movie['name'], movie['type'], movie['region'], movie['year'],
                                movie['actors'], movie['directors'], movie['description'], 
                                movie['url'], movie['dyid']
                            ))
                            updated_movies += 1
                        else:
                            # æ’å…¥æ–°å½±ç‰‡
                            cursor.execute("""
                            INSERT INTO dy (dyid, name, type, region, year, actors, directors, description, url)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, (
                                movie['dyid'], movie['name'], movie['type'], movie['region'], 
                                movie['year'], movie['actors'], movie['directors'], 
                                movie['description'], movie['url']
                            ))
                            new_movies += 1
                
                if m3u8s:
                    # æ‰¹é‡æ’å…¥æˆ–æ›´æ–°m3u8ä¿¡æ¯ï¼ˆæ™ºèƒ½æŸ¥é‡ï¼‰
                    for m3u8 in m3u8s:
                        if m3u8['m3u8_url']:  # åªä¿å­˜æœ‰æ•ˆçš„m3u8é“¾æ¥
                            cursor.execute("""
                            SELECT id, m3u8_url FROM m3u8 
                            WHERE dyid = ? AND episode = ?
                            """, (m3u8['dyid'], m3u8['episode']))
                            existing = cursor.fetchone()
                            
                            if existing:
                                # å¦‚æœå·²å­˜åœ¨ä½†m3u8_urlä¸ºç©ºï¼Œåˆ™æ›´æ–°
                                if not existing[1]:  # m3u8_urlä¸ºç©º
                                    cursor.execute("""
                                    UPDATE m3u8 SET 
                                        name = ?, play_url = ?, m3u8_url = ?, crawl_time = CURRENT_TIMESTAMP
                                    WHERE dyid = ? AND episode = ?
                                    """, (m3u8['name'], m3u8['play_url'], m3u8['m3u8_url'], 
                                         m3u8['dyid'], m3u8['episode']))
                                    updated_m3u8s += 1
                                # å¦‚æœå·²å­˜åœ¨ä¸”æœ‰m3u8_urlï¼Œè·³è¿‡ï¼ˆé¿å…é‡å¤ï¼‰
                            else:
                                # æ’å…¥æ–°è®°å½•
                                cursor.execute("""
                                INSERT INTO m3u8 (dyid, name, episode, play_url, m3u8_url)
                                VALUES (?, ?, ?, ?, ?)
                                """, (m3u8['dyid'], m3u8['name'], m3u8['episode'], 
                                     m3u8['play_url'], m3u8['m3u8_url']))
                                new_m3u8s += 1
                
                self.conn.commit()
                
                # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                if new_movies or updated_movies or new_m3u8s or updated_m3u8s:
                    stats = []
                    if new_movies: stats.append(f"æ–°å¢{new_movies}éƒ¨å½±ç‰‡")
                    if updated_movies: stats.append(f"æ›´æ–°{updated_movies}éƒ¨å½±ç‰‡")
                    if new_m3u8s: stats.append(f"æ–°å¢{new_m3u8s}ä¸ªm3u8")
                    if updated_m3u8s: stats.append(f"è¡¥å……{updated_m3u8s}ä¸ªm3u8")
                    print(f"ğŸ’¾ æ‰¹é‡ä¿å­˜: {', '.join(stats)}")
                
                return True
                
            except Exception as e:
                print(f"æ‰¹é‡ä¿å­˜æ•°æ®å¤±è´¥: {e}")
                self.conn.rollback()
                return False
            finally:
                cursor.close()
    
    def add_to_batch(self, movie_info=None, m3u8_info=None):
        """æ·»åŠ æ•°æ®åˆ°æ‰¹é‡å¤„ç†é˜Ÿåˆ—"""
        with self.batch_lock:
            if movie_info:
                self.movie_batch.append(movie_info)
            if m3u8_info:
                self.m3u8_batch.extend(m3u8_info)
            
            # å½“æ‰¹é‡è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œæ‰§è¡Œä¿å­˜
            if (len(self.movie_batch) >= self.batch_size or 
                len(self.m3u8_batch) >= self.batch_size * 5):
                self.flush_batch()
    
    def flush_batch(self):
        """åˆ·æ–°æ‰¹é‡æ•°æ®åˆ°æ•°æ®åº“"""
        if self.movie_batch or self.m3u8_batch:
            movies = self.movie_batch.copy()
            m3u8s = self.m3u8_batch.copy()
            self.movie_batch.clear()
            self.m3u8_batch.clear()
            
            success = self.batch_save_to_db(movies, m3u8s)
            if success:
                print(f"æ‰¹é‡ä¿å­˜: {len(movies)}éƒ¨å½±ç‰‡, {len(m3u8s)}ä¸ªm3u8é“¾æ¥")
            return success
        return True
    
    def crawl_movie_fast(self, url):
        """å¿«é€Ÿçˆ¬å–å•éƒ¨å½±ç‰‡ï¼ˆå¸¦æŸ¥é‡åŠŸèƒ½ï¼‰"""
        try:
            # æå–dyidè¿›è¡Œé¢„æ£€æŸ¥
            match = re.search(r'/mp4/(\d+)\.html', url)
            if not match:
                print(f"âŒ æ— æ³•ä»URLæå–dyid: {url}")
                return False
            
            dyid = int(match.group(1))
            
            # æ£€æŸ¥å½±ç‰‡æ˜¯å¦å·²å­˜åœ¨
            movie_exists = self.check_movie_exists(dyid)
            
            # è·å–å½±ç‰‡è¯¦æƒ…é¡µé¢
            response = self._get_with_retry(url, timeout=5)
            if not response:
                return False
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # è·å–é›†æ•°
            episode_count = self.get_episode_count_fast(soup)
            
            movie_info = None
            m3u8_data = []
            
            # è·å–å½±ç‰‡åç§°ï¼ˆç”¨äºm3u8è®°å½•ï¼‰
            movie_name = f"å½±ç‰‡{dyid}"  # é»˜è®¤åç§°
            
            # å¦‚æœå½±ç‰‡ä¸å­˜åœ¨ï¼Œéœ€è¦çˆ¬å–å½±ç‰‡ä¿¡æ¯
            if not movie_exists:
                movie_info = self.parse_movie_detail_fast(url)
                if not movie_info:
                    return False
                movie_name = movie_info['name']
                print(f"ğŸ†• æ–°å½±ç‰‡: {movie_name}")
            else:
                # å½±ç‰‡å·²å­˜åœ¨ï¼Œä»æ•°æ®åº“è·å–åç§°
                with self.db_lock:
                    cursor = self.conn.cursor()
                    try:
                        cursor.execute("SELECT name FROM dy WHERE dyid = ?", (dyid,))
                        result = cursor.fetchone()
                        if result:
                            movie_name = result[0]
                    finally:
                        cursor.close()
                print(f"ğŸ“‹ å·²å­˜åœ¨å½±ç‰‡ID: {dyid} ({movie_name})")
            
            # æ£€æŸ¥m3u8é“¾æ¥æƒ…å†µ
            missing_episodes = self.get_missing_episodes(dyid, episode_count)
            
            if missing_episodes:
                print(f"ğŸ” éœ€è¦è¡¥å…… {len(missing_episodes)} é›†: {missing_episodes}")
                # åªçˆ¬å–ç¼ºå¤±çš„é›†æ•°
                m3u8_data = self.get_m3u8_urls_selective(
                    dyid, missing_episodes, movie_name
                )
            else:
                print(f"âœ… æ‰€æœ‰é›†æ•°å·²å®Œæ•´: {episode_count}é›†")
            
            # æ·»åŠ åˆ°æ‰¹é‡å¤„ç†é˜Ÿåˆ—
            if movie_info or m3u8_data:
                self.add_to_batch(movie_info, m3u8_data)
                
                valid_m3u8_count = sum(1 for m in m3u8_data if m['m3u8_url']) if m3u8_data else 0
                status = "æ–°å¢" if movie_info else "è¡¥å……"
                print(f"âœ“ {status} ({episode_count}é›†, {valid_m3u8_count}ä¸ªæ–°é“¾æ¥)")
            
            return True
            
        except Exception as e:
            print(f"çˆ¬å–å½±ç‰‡å¤±è´¥ {url}: {e}")
            return False
    
    def save_progress(self, category, current_page, total_pages, last_dyid, status="running"):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        with self.db_lock:
            cursor = self.conn.cursor()
            try:
                cursor.execute("SELECT id FROM crawl_progress WHERE category = ?", (category,))
                if cursor.fetchone():
                    cursor.execute("""
                    UPDATE crawl_progress SET 
                        current_page = ?, total_pages = ?, last_dyid = ?, 
                        status = ?, update_time = CURRENT_TIMESTAMP
                    WHERE category = ?
                    """, (current_page, total_pages, last_dyid, status, category))
                else:
                    cursor.execute("""
                    INSERT INTO crawl_progress (category, current_page, total_pages, last_dyid, status)
                    VALUES (?, ?, ?, ?, ?)
                    """, (category, current_page, total_pages, last_dyid, status))
                
                self.conn.commit()
                return True
            except Exception as e:
                print(f"ä¿å­˜çˆ¬å–è¿›åº¦å¤±è´¥: {e}")
                self.conn.rollback()
                return False
            finally:
                cursor.close()
    
    def get_progress(self, category):
        """è·å–æŒ‡å®šåˆ†ç±»çš„çˆ¬å–è¿›åº¦"""
        with self.db_lock:
            cursor = self.conn.cursor()
            cursor.execute("""
            SELECT category, current_page, total_pages, last_dyid, status
            FROM crawl_progress WHERE category = ?
            """, (category,))
            
            row = cursor.fetchone()
            cursor.close()
            if row:
                return dict(row)
            return None
    
    def crawl_category_optimized(self, category_id, start_page=None):
        """ä¼˜åŒ–çš„åˆ†ç±»çˆ¬å–æ–¹æ³•"""
        category_name = CATEGORIES.get(category_id, f"åˆ†ç±»{category_id}")
        print(f"ğŸš€ å¼€å§‹é«˜é€Ÿçˆ¬å–{category_name}...")
        
        current_page = 1
        total_pages = 0
        
        try:
            # è·å–æˆ–æ¢å¤è¿›åº¦
            progress = self.get_progress(category_id)
            if progress and progress['status'] == 'running' and start_page is None:
                current_page = progress['current_page']
                total_pages = progress['total_pages']
                print(f"ğŸ“‹ æ¢å¤çˆ¬å–è¿›åº¦: å½“å‰é¡µ {current_page}/{total_pages}")
            else:
                total_pages = self.get_total_pages(category_id)
                if total_pages == 0:
                    print(f"âŒ è·å–{category_name}æ€»é¡µæ•°å¤±è´¥")
                    return False
                current_page = 1 if start_page is None else start_page
                print(f"ğŸ“Š {category_name}æ€»é¡µæ•°: {total_pages}")
            
            if self.test_mode and total_pages > 2:
                total_pages = 2
                print("ğŸ§ª æµ‹è¯•æ¨¡å¼: åªçˆ¬å–å‰2é¡µ")
            
            self.save_progress(category_id, current_page, total_pages, 0, "running")
            
            # æŒ‰æ‰¹æ¬¡å¤„ç†é¡µé¢
            batch_size = min(3, total_pages - current_page + 1)  # æ¯æ‰¹å¤„ç†3é¡µ
            
            for batch_start in range(current_page, total_pages + 1, batch_size):
                batch_end = min(batch_start + batch_size - 1, total_pages)
                batch_pages = list(range(batch_start, batch_end + 1))
                
                print(f"ğŸ“¦ æ‰¹é‡å¤„ç†é¡µé¢ {batch_start}-{batch_end}")
                
                # æ‰¹é‡è·å–å½±ç‰‡é“¾æ¥
                movie_links = self.get_movie_links_batch(category_id, batch_pages)
                print(f"ğŸ”— è·å–åˆ° {len(movie_links)} ä¸ªå½±ç‰‡é“¾æ¥")
                
                if not movie_links:
                    continue
                
                # å¹¶å‘çˆ¬å–å½±ç‰‡
                success_count = 0
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    future_to_url = {
                        executor.submit(self.crawl_movie_fast, url): url 
                        for url in movie_links
                    }
                    
                    # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
                    with tqdm(total=len(movie_links), desc=f"çˆ¬å–è¿›åº¦") as pbar:
                        for future in as_completed(future_to_url):
                            url = future_to_url[future]
                            try:
                                if future.result():
                                    success_count += 1
                            except Exception as e:
                                print(f"âŒ å¤„ç†å¤±è´¥ {url}: {e}")
                            finally:
                                pbar.update(1)
                                self._smart_delay()
                
                # åˆ·æ–°æ‰¹é‡æ•°æ®
                self.flush_batch()
                
                # æ›´æ–°è¿›åº¦
                self.save_progress(category_id, batch_end, total_pages, 0, "running")
                
                print(f"âœ… æ‰¹æ¬¡å®Œæˆ: {success_count}/{len(movie_links)} æˆåŠŸ")
            
            # æœ€ç»ˆåˆ·æ–°
            self.flush_batch()
            self.save_progress(category_id, total_pages, total_pages, 0, "completed")
            print(f"ğŸ‰ {category_name}çˆ¬å–å®Œæˆ!")
            return True
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸ çˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­")
            self.flush_batch()
            self.save_progress(category_id, current_page, total_pages, 0, "interrupted")
            return False
        except Exception as e:
            print(f"ğŸ’¥ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.flush_batch()
            self.save_progress(category_id, current_page, total_pages, 0, "error")
            return False
    
    def crawl_all_optimized(self):
        """ä¼˜åŒ–çš„å…¨åˆ†ç±»çˆ¬å–"""
        print("ğŸš€ å¼€å§‹é«˜é€Ÿçˆ¬å–æ‰€æœ‰åˆ†ç±»...")
        for category_id in CATEGORIES:
            if not self.crawl_category_optimized(category_id):
                print("âš ï¸ çˆ¬å–è¢«ä¸­æ–­æˆ–å‡ºé”™ï¼Œåœæ­¢æ‰€æœ‰çˆ¬å–ä»»åŠ¡ã€‚")
                break
            print(f"â±ï¸ ç­‰å¾… {self.delay * 2} ç§’åç»§ç»­ä¸‹ä¸€ä¸ªåˆ†ç±»...")
            time.sleep(self.delay * 2)
    
    def close(self):
        """å…³é—­èµ„æº"""
        self.flush_batch()  # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å·²ä¿å­˜
        if self.session:
            self.session.close()
        if self.conn:
            self.conn.close()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DSQ4Då½±è§†èµ„æºé«˜é€Ÿçˆ¬è™«")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼Œæ¯ä¸ªåˆ†ç±»åªçˆ¬å–å‰2é¡µ")
    parser.add_argument("--category", type=int, choices=list(CATEGORIES.keys()), help="æŒ‡å®šè¦çˆ¬å–çš„åˆ†ç±»ID (1=ç”µå½±, 2=ç”µè§†å‰§, 3=åŠ¨æ¼«, 4=ç»¼è‰º, 19=å¤§é™†å‰§, 20=æ¬§ç¾å‰§, 21=é¦™æ¸¯å‰§, 22=éŸ©å›½å‰§, 23=å°æ¹¾å‰§, 24=æ—¥æœ¬å‰§, 25=æµ·å¤–å‰§, 26=æ³°å›½å‰§, 27=çŸ­å‰§)")
    parser.add_argument("--page", type=int, help="æŒ‡å®šä»å“ªä¸€é¡µå¼€å§‹çˆ¬å–")
    parser.add_argument("--delay", type=float, default=0.1, help="è¯·æ±‚å»¶è¿Ÿæ—¶é—´(ç§’)")
    parser.add_argument("--workers", type=int, default=8, help="å¹¶å‘çº¿ç¨‹æ•°")
    parser.add_argument("--batch-size", type=int, default=50, help="æ‰¹é‡å¤„ç†å¤§å°")
    
    args = parser.parse_args()
    
    print("ğŸ¬ DSQ4Dé«˜é€Ÿçˆ¬è™«å¯åŠ¨ä¸­...")
    print(f"âš™ï¸ é…ç½®: å¹¶å‘æ•°={args.workers}, å»¶è¿Ÿ={args.delay}s, æ‰¹é‡å¤§å°={args.batch_size}")
    
    crawler = OptimizedDSQ4DCrawler(
        test_mode=args.test,
        delay=args.delay,
        max_workers=args.workers,
        batch_size=args.batch_size
    )
    
    try:
        if args.category:
            crawler.crawl_category_optimized(args.category, args.page)
        else:
            crawler.crawl_all_optimized()
    finally:
        crawler.close()
        print("ğŸ çˆ¬è™«å·²å…³é—­")

if __name__ == "__main__":
    main()